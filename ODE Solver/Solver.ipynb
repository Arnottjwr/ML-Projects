{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from autograd import grad, elementwise_grad\n",
    "import autograd.numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "from net_visualiser import DrawNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem\n",
    "We want to solve the following BVP $$y'' = -2,\\quad0<x<1$$ $$y(0) = y(1) = 1$$ using a neural network. This BVP has analytic solution $$y = 1 + x(1-x)$$ which we will use to evaluate our solution. We first define the loss function we shall use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "In order to train our neural network, we use gradient descent. This is simple repeated calculation of the form $$\\theta_{n+1} = \\theta_n-\\alpha\\nabla_\\theta L$$ where $L = L(x_k,\\theta)$ is the loss function and $\\theta$ are the parameters. How do we calculate $\\nabla_\\theta L$? For a network of $N$ layers, this vector is given $$\\nabla_\\theta L = \\left(\\frac{\\partial L}{\\partial W^{(1)}},\\dots,\\frac{\\partial L}{\\partial W^{(N)}}, \\frac{\\partial L}{\\partial b^{(1)}},\\dots,\\frac{\\partial L}{\\partial b^{(N)}}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_params(architecture): # will generalise later\n",
    "    W1 = np.random.randn(architecture[1], architecture[0])* 0.01\n",
    "    b1 = np.random.randn(architecture[1], 1)\n",
    "    return [W1, b1]\n",
    "\n",
    "def neural_network(params,x, architecture):\n",
    "    params = initialise_params(architecture)\n",
    "\n",
    "def y_trial(x, params):\n",
    "    return x*(1 - x)*neural_network(x,params) + 1\n",
    "\n",
    "def y(x_k, params,activ_function = sigmoid):\n",
    "    W = params[0]\n",
    "    b = params[1]\n",
    "    return activ_function(W@x_k + b)\n",
    "\n",
    "\n",
    "def loss(x_k, params):\n",
    "    dy = elementwise_grad(y,0)\n",
    "    ddy = elementwise_grad(dy,0)(x_k, params)\n",
    "    return np.sum((ddy + 2*np.ones_like(ddy))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic(x):\n",
    "    return 1 + x*(1-x)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def initialise_params(x): # will generalise later\n",
    "    n = len(x)\n",
    "    W1 = np.random.randn(n, n)#* 0.01\n",
    "    b1 = np.random.randn(n, 1)\n",
    "    return [W1, b1]\n",
    "\n",
    "def loss(params, x,activ_function = sigmoid):\n",
    "    # unpack\n",
    "    W = params[0]\n",
    "    b = np.reshape(params[1],(len(x),))\n",
    "    # single layer\n",
    "    y_hat = activ_function(W@x + b)\n",
    "    ## preprocess\n",
    "    left_bc_vec = np.zeros_like(x)\n",
    "    left_bc_vec[0] = 1 - y_hat[0]#._value\n",
    "    right_bc_vec = np.zeros_like(x)\n",
    "    right_bc_vec[-1] = 1 - y_hat[-1]#._value\n",
    "    y = lambda x: y_hat + (np.ones_like(x)-x)*left_bc_vec + x*right_bc_vec\n",
    "    dy = elementwise_grad(y)\n",
    "    ddy = elementwise_grad(dy,0)(x)\n",
    "    return np.sum((ddy + 2*np.ones_like(ddy))**2)\n",
    "\n",
    "def solve_ode_neural_network(x, epochs, learning_rate, activ_function=sigmoid,):\n",
    "    costs = []\n",
    "    ## Initialise Parameters\n",
    "    params = initialise_params(x)\n",
    "    for e in range(epochs):\n",
    "        # print(f'Epoch:{e}')\n",
    "        W = params[0]\n",
    "        b = params[1]\n",
    "        ## backpropagation\n",
    "        loss_grad = grad(loss)\n",
    "\n",
    "        ## Compute loss\n",
    "        cost = loss(params, x)\n",
    "        if e % 100 == 0 or i == epochs:\n",
    "            costs.append(cost)\n",
    "        for i in range(100):\n",
    "            # Compute gradients\n",
    "            gradients = loss_grad(params, x)\n",
    "            dW, db = gradients\n",
    "\n",
    "        # Update parameters using gradient descent\n",
    "            params[0] -= learning_rate * dW\n",
    "            params[1] -= learning_rate * db\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    return params\n",
    "\n",
    "def net_approximation(x, final_params, activ_function = sigmoid):\n",
    "    W = final_params[0]\n",
    "    b = np.reshape(final_params[1],(len(x),))\n",
    "    return activ_function(W@x + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approximation\n",
    "N = 50\n",
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "x = np.linspace(0,1,N)\n",
    "params = solve_ode_neural_network(x,epochs, learning_rate)\n",
    "y_n = net_approximation(x, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters\n",
    "fig, ax = plt.subplots(1,2,figsize = (12,7))\n",
    "ax[0].plot(np.linspace(0,1,1000),analytic(np.linspace(0,1,1000)),label = 'Analytic Solution')\n",
    "ax[0].plot(x,y_n,label = 'Approximation')\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylabel('y(x)')\n",
    "ax[0].set_title('Solution')\n",
    "ax[0].grid()\n",
    "ax[0].legend()\n",
    "ax[1].set_title('Error')\n",
    "# final error, NN loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(x_k, architecture, activ_function, epochs, learning_rate):\n",
    "    params = initialise_params(architecture)\n",
    "    for _ in range(epochs):\n",
    "        W = params[0]\n",
    "        b = params[1]\n",
    "        \n",
    "        loss_func_grad = grad(loss,0)\n",
    "        ## Backward prop\n",
    "        for i in range(100): # make parameter\n",
    "            cost_grad =  loss_func_grad(x_k, params)\n",
    "            params[0] = params[0] - learning_rate * cost_grad[0]\n",
    "            params[1] = params[1] - learning_rate * cost_grad[1]\n",
    "    return params\n",
    "\n",
    "def net_approximation(x_k, final_params):\n",
    "    return y(x_k,final_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DrawNN(architecture).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(x_k, architecture, activ_function, epochs, learning_rate):\n",
    "    ## Intialise Params\n",
    "    params = initialise_params(architecture)\n",
    "    ## Begin Training \n",
    "    for _ in range(epochs):\n",
    "        \n",
    "        ## Forward Prop - make forward prop function\n",
    "        y_k = single_layer_network(x_k, activ_function, params)\n",
    "\n",
    "        ## Post process\n",
    "        # y = post_process(y_k, x_k)\n",
    "        W = params[0]\n",
    "        b = params[1]\n",
    "        def y(x_k):\n",
    "            y_k = activ_function(W@x_k + b)\n",
    "            return y_k + (1-x_k)*(y_k[0] - 1) + x_k*(y_k[-1] - 1)\n",
    "        \n",
    "        ## Compute derivatives\n",
    "        y_x = elementwise_grad(y)\n",
    "        y_xx = elementwise_grad(y_x)(x_k)\n",
    "        \n",
    "        ## Calculate Loss\n",
    "        L = loss(y_xx,x_k)\n",
    "        loss_func_grad = grad(loss,0)\n",
    "        ## Backward prop\n",
    "        for i in range(100): # make parameter\n",
    "            cost_grad =  loss_func_grad(params, x_k)\n",
    "            params[0] = params[0] - learning_rate * cost_grad[0]\n",
    "            params[1] = params[1] - learning_rate * cost_grad[1]\n",
    "\n",
    "        ## Backward prop\n",
    "    return params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
